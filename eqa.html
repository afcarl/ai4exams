
<head>
	<link rel="stylesheet" type="text/css" href="eqa.css">
</head>

<div id="content">

<div id="center-nav">
<ul>
  <li><a href="workshop.html">WORKSHOP</a></li>
  <li><a href="papers.html">PAPERS</a></li>
  <li><a href="data.html">DATA</a></li>
  <li class="active">MAIN</a></li>
  <li><a href="tools.html">TOOLS</a></li>
  <li><a href="results.html">RESULTS</a></li>
  <li><a href="contact.html">CONTACT</a></li>
  </ul>
  <hr width="70%">
</div>



<div id="text-passage">
<div id="logo">
  <span id="logo-big">E</span>xam<br>
  <span id="logo-big">Q</span>uestion<br>
  <span id="logo-big">A</span>nswer
</div>
<h3> <span id="h3-big">W</span>hat is exam question answering? </h3>

We've created this website as community resource for researchers interested in machine intelligence, particularly in systems developed for or evaluated in an exam question answering (EQA) setting.  We define EQA as:
<center>
<div id="quote">
<i>A machine reading task in which the system is provided a set of texts (sometimes with visual aids) and a corresponding set of questions.  The system is successful if it can process the text and provide the correct answers to the questions. </i> 
</div> 
</center>
 These questions can be multiple choice, in which case a set of answer candidates is given, or short answer, where they are not.  Recent work <sup>1, 2</sup>
<br>
</div>



<div id="text-passage">
<h3> <span id="h3-big">W</span>hy?</h3>

As artificial intelligence has matured as a field, many researchers have attempted to define what it means for a machine to be intelligent, and how to measure our progress in creating intelligent machines.  While the Turing test has emerged as the most notable, it is a poor fit for quickly and comparatively evaluating how well a machine can read and reason.
<br>
In recent years, more attention has been placed on the use of exams, quizzes, as word games as a measure of these skills, and systems such as IBM's Watson system outperformed human competitors on Jeopardy!
</div>
<br>
<br>
</div>
</div>