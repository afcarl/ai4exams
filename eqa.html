
<head>
	<link rel="stylesheet" type="text/css" href="eqa.css">
</head>

<div id="content">

<div id="center-nav">
<ul>
  <li><a href="workshop.html">WORKSHOP</a></li>
  <li><a href="papers.html">PAPERS</a></li>
  <li><a href="data.html">DATA</a></li>
  <li class="active">MAIN</a></li>
  <li><a href="tools.html">TOOLS</a></li>
  <li><a href="results.html">RESULTS</a></li>
  <li><a href="contact.html">CONTACT</a></li>
  </ul>
  <hr width="70%">
</div>



<div id="text-passage">
<div id="logo">
  <span id="logo-big">E</span>xam<br>
  <span id="logo-big">Q</span>uestion<br>
  <span id="logo-big">A</span>nswer
</div>
</div>

<h3> <span id="h3-big">W</span>hat is exam question answering? </h3>
<div id="text-passage">

 Exam question answering (EQA) is a variant of question answering, an emerging task in NLP, spurred on from an increasing amount of recent work in end-to-end learning.  We define EQA as:
<center>
<div id="quote">
<i>A machine reading task in which the system is provided a set of texts (sometimes with visual aids) and a corresponding set of questions.  The system is successful if it can process the text and provide the correct answers to the questions. </i> 
</div> 
</center>
 These questions can be multiple choice, in which case a set of answer candidates is given, or short answer, in which case they are not.  

<p>
 Most question answering tasks can be phrased as EQA, and as such most modeling improvements to QA systems are also be relevant to EQA.  However, what sets EQA apart is the use of standard, human-curated exams and resources.  Because these exams are also administered to humans, their use provides not only a point of comparison between other machine reading systems, but also a benchmark of machine reading vs. human intelligence.
</p>

<!-- Recent work <sup>1, 2</sup> -->

 <p>
 This website serves as a community resource for researchers interested in machine intelligence, particularly in systems developed for, or evaluated in, an exam question answering (EQA) setting.  Here you will find a curated list of relevant papers, data sets, and comparative result tables.  In addition, we provide tools for unifying data and evaluation across different QA models.
 </p>
</div>



<h3> <span id="h3-big">M</span>otivation</h3>
<div id="text-passage">
<p>
As artificial intelligence has matured as a field, researchers have long sought to define what it means for a machine to be intelligent and to devise a test with which to measure our progress towards creating intelligent machines.  While the Turing test has emerged as the most notable and widely-cited of these measures, it is a poor fit for the research environment where a quick and objective measure of a machine's reading and reasoning capabilities is needed.
<!--quickly and comparatively evaluating how well a machine can read and reason. -->
</p>
<p>
In recent years, more attention has been placed on the use of exams, quizzes, and word games as a measure of these skills.  For the same reasons that we subject ourselves to standardized testing -- to probe what knowledge we've memorized and what new facts we can infer -- we feel that EQA is an excellent benchmark to assess machine intelligence in NLP.  While simple tactics can produce high-scoring baselines <sup>[<a href="results.html#richardson2013">1</a>]</sup>, it stands to reason that matching expert human performance on exams will require machines to comprehend the same facts, and to make similar inferences.
</p>
<!-- Just as Kasparov's defeat at the hands of IBM's Deep Blue,  and systems such as IBM's Watson system outperformed human competitors on Jeopardy!  -->
</p>
<br>

<hr width="100%">

<div id="main-footer">

<div style="float: left; width: 95px; padding-right: 1em;"><img src="_images/ucl-logo.png" height="30px"></div>

<div style="float: left; width: 150px; padding-right:1em;"><img src="_images/nii.gif" height="30px"></div>

<div style="float: left;"><img src="_images/daiwa.jpeg" width="90px"></div>

</div>
<br>
</div>
<br>
<br>

</div>
</div>








